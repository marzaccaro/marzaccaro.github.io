

[
  
  
    
    
      {
        "title": "How to tell a story with data",
        "excerpt": "The inspiration\n\nIt all started with a workshop on how to create an effective chart by Cole Nussbaumer Knaflic at the Tableau User Group London. It was so inspiring and captivating, she managed to engage the public asking for their own recommendations and ideas. I thought to do the same in my company, as we have a big data team and we encourage the business to use a self-serve platform to answer their own data questions.\n\nTo start with, I read again Storytelling with data. The book is a great starting point for anyone who wants to enhance their ability to use data visualisation and storytelling to bring people to action, once they have a good point supported by data. It was the second time I read it, and every time its clarity and effectiveness amaze me.\n\nThe planning\n\nI wanted to copy the same structure she used, but then I felt my level wasn’t quite the same and I wanted to add my own personal experience to the presentation, to make it more compelling. Like she illustrates in her book I started off with a bunch of post-it on a wall: creating the structure of your presentation with sticky notes or on a whiteboard, you don’t grow attached to your work and you have more flexibility and freedom to arrange it.\n\n\n  \n  Storyboarding\n\n\nAfter this exercise I knew the outline of my presentation:\n\n\n  Why storytelling with data\n  How to tell a good story with your data using the steps from the book\n  Workshop / practical part\n\n\nThe final presentation\n\nOnly then, I start refining the sections one by one. For the why I wanted to highlight why using a story is effective and the power of data visualisation to see patterns and trends at a glance. In the how to I wanted to follow the principles in the book Storytelling with data:\n&lt;/p&gt;\n\n\n  Understand the context\n  Choose an appropriate display\n  Eliminate clutter\n  Draw attention where you want it\n  Think like a designer\n  Tell a story\n\n\nThe main section has to be the 2, which includes a selection of widely used charts and when to use them. The final result was well received and – I hope – increased awareness around data visualisation when presenting a business case.\n\nThe final result is below.\n\n\n\n\n\n  \n\n\n",
        "content": "The inspiration\n\nIt all started with a workshop on how to create an effective chart by Cole Nussbaumer Knaflic at the Tableau User Group London. It was so inspiring and captivating, she managed to engage the public asking for their own recommendations and ideas. I thought to do the same in my company, as we have a big data team and we encourage the business to use a self-serve platform to answer their own data questions.\n\nTo start with, I read again Storytelling with data. The book is a great starting point for anyone who wants to enhance their ability to use data visualisation and storytelling to bring people to action, once they have a good point supported by data. It was the second time I read it, and every time its clarity and effectiveness amaze me.\n\nThe planning\n\nI wanted to copy the same structure she used, but then I felt my level wasn’t quite the same and I wanted to add my own personal experience to the presentation, to make it more compelling. Like she illustrates in her book I started off with a bunch of post-it on a wall: creating the structure of your presentation with sticky notes or on a whiteboard, you don’t grow attached to your work and you have more flexibility and freedom to arrange it.\n\n\n  \n  Storyboarding\n\n\nAfter this exercise I knew the outline of my presentation:\n\n\n  Why storytelling with data\n  How to tell a good story with your data using the steps from the book\n  Workshop / practical part\n\n\nThe final presentation\n\nOnly then, I start refining the sections one by one. For the why I wanted to highlight why using a story is effective and the power of data visualisation to see patterns and trends at a glance. In the how to I wanted to follow the principles in the book Storytelling with data:\n&lt;/p&gt;\n\n\n  Understand the context\n  Choose an appropriate display\n  Eliminate clutter\n  Draw attention where you want it\n  Think like a designer\n  Tell a story\n\n\nThe main section has to be the 2, which includes a selection of widely used charts and when to use them. The final result was well received and – I hope – increased awareness around data visualisation when presenting a business case.\n\nThe final result is below.\n\n\n\n\n\n  \n\n\n",
        "url": "/2019/02/10/how-to-tell-a-story-with-data/"
      },
    
      {
        "title": "Edward Tufte and the principle of Graphical Integrity",
        "excerpt": "My favourite meet up in London\n\nOften you learn more talking with people and sharing your experience than studying on your own. In London, there are plenty of opportunities for networking and knowledge sharing and my favourite place is definitely Tiny Tableau Talks, a selection of 5 minutes talks about data visualisation and storytelling. I love it so much because:\n\n\n  In 5 minutes you have to keep it short and often you go straight to the core concept\n  Most of the talks are useful tips\n  It is open to anyone, not just the experts, as long as you have something interesting to share\n  Even if most of the talks are about Tableau, the lessons you learn are often generic and applicable to any data visualisation tool you’re using\n  The organisers are great and help you to get ready to give a talk.\n\n\nMy experience\n\nI’ve been to some of these meet-ups and learned so much, that I felt I have to give back to the community and do my own tiny talk. I am particularly fascinated by history in general because I like to know how and why things are the way they are, when did they change, when people started to formalise some concepts and why? Data visualisation is not different, and despite being a recent field it has an awesome history to discover.\n\nChoosing the subject\n\nOne of the pioneers in data visualisation and graphical representation of information is Edward Tufte, and I loved his masterpiece The display of quantitative information, which is included by Amazon in the top 100 books of the 20th century.\n\n\n  \n  The Visual Display of Quantitative Information by E. Tufte\n\n\nHe formalised a lot of principles of Data Visualisation before digital tools were so popular and mostly journalists were using charts to make news more informative.\n\nI then asked myself: which of Tufte’s lessons – born in paper design – are still applicable in digital design?\n\nThe principle of graphical integrity\n\nThe first principle illustrated by Tufte is the principle of graphical integrity, which is pushing for truthfulness in data representation. The core elements are:\n\n\n  Graphic should respect the proportions of the numbers they represent\n  Use labelling to help understanding\n  Show data variation, not design variation\n  When displaying monetary variation over time, is better to standardise the units (e.g. consider inflation)\n  Don’t use more dimensions than the ones included in the chart (avoid 3D)\n  Don’t quote data out of context\n\n\nThe content of my presentation\n\nI couldn’t include all these in 5 minutes, so I cut down to three points and included a practical example for each of them. You can find the final result in the video below.\n\n\n\nMy thoughts on Tufte’s book\n\nIn my opinion, the book is addressing mainly journalists and designer, that’s why there is such a prescriptive approach. Nonetheless – especially now that we are bombarded with reading opportunities – journalists and designers need to capture the eyes of the reader, with both truthful and visually appealing representations.\n\nMore learning along the way\n\nGoogling around Data Visualisation History you can find this amazing project to outline a timeline with milestones of the history of data visualization, by Michael Friendly along with his books, papers and presentations. Here are some curiosities: the bar, line and pie chart were all introduced by William Playfair in the 18th century, whereas Tufte is the inventor of the sparkline, that was first used in 2004.\n",
        "content": "My favourite meet up in London\n\nOften you learn more talking with people and sharing your experience than studying on your own. In London, there are plenty of opportunities for networking and knowledge sharing and my favourite place is definitely Tiny Tableau Talks, a selection of 5 minutes talks about data visualisation and storytelling. I love it so much because:\n\n\n  In 5 minutes you have to keep it short and often you go straight to the core concept\n  Most of the talks are useful tips\n  It is open to anyone, not just the experts, as long as you have something interesting to share\n  Even if most of the talks are about Tableau, the lessons you learn are often generic and applicable to any data visualisation tool you’re using\n  The organisers are great and help you to get ready to give a talk.\n\n\nMy experience\n\nI’ve been to some of these meet-ups and learned so much, that I felt I have to give back to the community and do my own tiny talk. I am particularly fascinated by history in general because I like to know how and why things are the way they are, when did they change, when people started to formalise some concepts and why? Data visualisation is not different, and despite being a recent field it has an awesome history to discover.\n\nChoosing the subject\n\nOne of the pioneers in data visualisation and graphical representation of information is Edward Tufte, and I loved his masterpiece The display of quantitative information, which is included by Amazon in the top 100 books of the 20th century.\n\n\n  \n  The Visual Display of Quantitative Information by E. Tufte\n\n\nHe formalised a lot of principles of Data Visualisation before digital tools were so popular and mostly journalists were using charts to make news more informative.\n\nI then asked myself: which of Tufte’s lessons – born in paper design – are still applicable in digital design?\n\nThe principle of graphical integrity\n\nThe first principle illustrated by Tufte is the principle of graphical integrity, which is pushing for truthfulness in data representation. The core elements are:\n\n\n  Graphic should respect the proportions of the numbers they represent\n  Use labelling to help understanding\n  Show data variation, not design variation\n  When displaying monetary variation over time, is better to standardise the units (e.g. consider inflation)\n  Don’t use more dimensions than the ones included in the chart (avoid 3D)\n  Don’t quote data out of context\n\n\nThe content of my presentation\n\nI couldn’t include all these in 5 minutes, so I cut down to three points and included a practical example for each of them. You can find the final result in the video below.\n\n\n\nMy thoughts on Tufte’s book\n\nIn my opinion, the book is addressing mainly journalists and designer, that’s why there is such a prescriptive approach. Nonetheless – especially now that we are bombarded with reading opportunities – journalists and designers need to capture the eyes of the reader, with both truthful and visually appealing representations.\n\nMore learning along the way\n\nGoogling around Data Visualisation History you can find this amazing project to outline a timeline with milestones of the history of data visualization, by Michael Friendly along with his books, papers and presentations. Here are some curiosities: the bar, line and pie chart were all introduced by William Playfair in the 18th century, whereas Tufte is the inventor of the sparkline, that was first used in 2004.\n",
        "url": "/2019/02/23/edward-tufte-and-the-graphical-integrity/"
      },
    
      {
        "title": "JOIN 2019, London",
        "excerpt": "What is Join\n\nJoin – Tour is a conference for Looker users touring around the World and I took part in the London session on the 9th of April at The Brewery. Looker is a self -service BI tool built for the cloud and, as many cloud services, they are building their own platform capability, building services and integrations to other analytics components, i.e. API integrations, actions, embedding etc.\n\n\n  \n  Join London at The Brewery\n\n\nThe structure of the conference\n\nThe structure of the conference was opening and closing keynotes, and then some talks in between, split in:\n\n\n  Business Cases, where Looker customers presented their success stories and how they used the tool in their organisation driving adoption and achievement. These are quite an interesting way to promote the tools, showing the real world scenario rather than a shiny demo.\n  Looker workshops, hands-on sessions to improve Looker knowledge thanks to the experts\n  Sessions on specific product features, again from the Looker experts, but this time with presenting the capabilities\n\n\nWhat is coming?\n\nI am most excited by the new visualisation capabilities, such as interactivity between Looks (Looker name for a query based report) and cross-filtering, but also the new dashboard filters design. Seems like the tool is moving towards more flexibility in term of visualisation.\n\nI was very pleased to see a talk specifically on visualisation design and considerations about which visualisations to pick depending on the question you’re trying to answer.\n\nWhere the analytics world is moving?\n\nI noticed two trends in the data and analytics space, not specific to Looker but generic: the data analyst as data evangelist and BI product as a platform.\n\n\n  \n    Data are like roads. Where we use roads to navigate to a location, we use data to navigate to a decision.The road analogy highlights the importance of joining up data. A single road only takes us to places between two locations; their real value comes from being part of a network. Data works in the same way: it is not just having more data that unlocks its value, but linking it together. Data is not individual datasets, it is a network\n  \n  \n  Jeni Tennison, Joining Up Data\n\n\nMore data and more systems mean more connections and the only way to interact with the plethora of SaaS applications that all digital companies use is to build connections between each of them to the others, so the users can flawless navigate and share information between these and enrich the vision of decision makers, avoiding silos where decision are made without the considering the bigger picture.\n\n\n  \n    Data analysis isn’t just about assembling, ordering and interpreting data; it’s also about educating, simplifying, clarifying, and persuading.\n  \n  \n  Eva Murray, Data Visualization And The Power Of Persuasion\n\n\nThe data analysts are no longer the heroes, providing reports and insights to the whole business, but they are driving business change and supporting more and more people to find the answer to their own questions. With the amount of data ever more increasing, is impossible to capture every data needs, so data analysts need to put in place an infrastructure that has a wide coverage and then empower more and more people to make data-driven decisions with self-service BI tools.\n",
        "content": "What is Join\n\nJoin – Tour is a conference for Looker users touring around the World and I took part in the London session on the 9th of April at The Brewery. Looker is a self -service BI tool built for the cloud and, as many cloud services, they are building their own platform capability, building services and integrations to other analytics components, i.e. API integrations, actions, embedding etc.\n\n\n  \n  Join London at The Brewery\n\n\nThe structure of the conference\n\nThe structure of the conference was opening and closing keynotes, and then some talks in between, split in:\n\n\n  Business Cases, where Looker customers presented their success stories and how they used the tool in their organisation driving adoption and achievement. These are quite an interesting way to promote the tools, showing the real world scenario rather than a shiny demo.\n  Looker workshops, hands-on sessions to improve Looker knowledge thanks to the experts\n  Sessions on specific product features, again from the Looker experts, but this time with presenting the capabilities\n\n\nWhat is coming?\n\nI am most excited by the new visualisation capabilities, such as interactivity between Looks (Looker name for a query based report) and cross-filtering, but also the new dashboard filters design. Seems like the tool is moving towards more flexibility in term of visualisation.\n\nI was very pleased to see a talk specifically on visualisation design and considerations about which visualisations to pick depending on the question you’re trying to answer.\n\nWhere the analytics world is moving?\n\nI noticed two trends in the data and analytics space, not specific to Looker but generic: the data analyst as data evangelist and BI product as a platform.\n\n\n  \n    Data are like roads. Where we use roads to navigate to a location, we use data to navigate to a decision.The road analogy highlights the importance of joining up data. A single road only takes us to places between two locations; their real value comes from being part of a network. Data works in the same way: it is not just having more data that unlocks its value, but linking it together. Data is not individual datasets, it is a network\n  \n  \n  Jeni Tennison, Joining Up Data\n\n\nMore data and more systems mean more connections and the only way to interact with the plethora of SaaS applications that all digital companies use is to build connections between each of them to the others, so the users can flawless navigate and share information between these and enrich the vision of decision makers, avoiding silos where decision are made without the considering the bigger picture.\n\n\n  \n    Data analysis isn’t just about assembling, ordering and interpreting data; it’s also about educating, simplifying, clarifying, and persuading.\n  \n  \n  Eva Murray, Data Visualization And The Power Of Persuasion\n\n\nThe data analysts are no longer the heroes, providing reports and insights to the whole business, but they are driving business change and supporting more and more people to find the answer to their own questions. With the amount of data ever more increasing, is impossible to capture every data needs, so data analysts need to put in place an infrastructure that has a wide coverage and then empower more and more people to make data-driven decisions with self-service BI tools.\n",
        "url": "/2019/04/15/join-2019-london/"
      },
    
      {
        "title": "A 3D exploding pie chart: not a good idea.",
        "excerpt": "A panel in a history of printing museum\n\nLast week I was visiting this museum about the history of invention and development of printing in Bassano Del Grappa, a small little village in the North of Italy. The museum was really well presented and it reminded me how the introduction of printing marked a revolution: more and more books were available and the knowledge was faster to spread.\n\nThe museum focus was about the Remondini’s: a famous family running a publishing business, particularly active in the ‘700 and based in Bassano. Historians knew it was flourishing, because it was the main reproducer of a satiric depiction called the Juicio Universal, that was widespread and talked about in all the Catholic countries. To point out how wide was the reach of the prints, a panel in the museum proposed this chart\n\n\n  \n  Panel at the Museo della Stampa Remondini, Bassano del Grappa\n\n\nAnd I decided to use this post as a commentary and an opportunity for a make over.\n\nWhy the 3D pie chart does not fit for purpose?\n\nLooking at the panel I find myself going back and forth between the chart and the legend, struggling to understand what the aim of the chart is and I actually start looking at Svizzera and then look back to the Impero because the colours are so similar. Here some more reasons why this chart is not a good choice:\n\n\n  Too many colours make it hard to decide which is which clearly\n  Spagna looks bigger than Stato Pontificio, even though the latter is 1% bigger in reality: this is due to the deformation caused by the 3D effect\n  At a glance I cannot understand what is going on and I need to read the whole legend.\n\n\nWhat are better options?\n\nWhat alternative can I use to display the data? Of course the answer is it depends! On what – I heard you asking? – on the message you’re trying to convey.\n\nIf it is important to compare how many of the reproductions were going in each region, I would use a bar chart, maybe highlighting the top region, like in the image below.\n\n\n  \n  Bar chart, the difference between the regions is now clear and there is no need for a legend.\n\n\nIf instead is important to mark that so many different regions were receiving these reproductions I would just list the number of different end markets and compare it to the number of existing regions at that time. Or alternatively give a sentence like X% of the number of books circulating in Europe where produced from Remondini, something along the line of this\n\n\n  \n  Sometimes a sentence with the main numbers is more effective. Note: these numbers are made up\n\n\nTo summarise the nature of this data-set makes it hard to choose a pie chart, both because the number of colours is too high and because some of the slices’ sizes are so similar. The 3D effect does not help – distorting the proportion of the real numbers. Curious to see when is a good idea to use a pie chart? Have a look at my presentation on how to better tell a story with data.\n",
        "content": "A panel in a history of printing museum\n\nLast week I was visiting this museum about the history of invention and development of printing in Bassano Del Grappa, a small little village in the North of Italy. The museum was really well presented and it reminded me how the introduction of printing marked a revolution: more and more books were available and the knowledge was faster to spread.\n\nThe museum focus was about the Remondini’s: a famous family running a publishing business, particularly active in the ‘700 and based in Bassano. Historians knew it was flourishing, because it was the main reproducer of a satiric depiction called the Juicio Universal, that was widespread and talked about in all the Catholic countries. To point out how wide was the reach of the prints, a panel in the museum proposed this chart\n\n\n  \n  Panel at the Museo della Stampa Remondini, Bassano del Grappa\n\n\nAnd I decided to use this post as a commentary and an opportunity for a make over.\n\nWhy the 3D pie chart does not fit for purpose?\n\nLooking at the panel I find myself going back and forth between the chart and the legend, struggling to understand what the aim of the chart is and I actually start looking at Svizzera and then look back to the Impero because the colours are so similar. Here some more reasons why this chart is not a good choice:\n\n\n  Too many colours make it hard to decide which is which clearly\n  Spagna looks bigger than Stato Pontificio, even though the latter is 1% bigger in reality: this is due to the deformation caused by the 3D effect\n  At a glance I cannot understand what is going on and I need to read the whole legend.\n\n\nWhat are better options?\n\nWhat alternative can I use to display the data? Of course the answer is it depends! On what – I heard you asking? – on the message you’re trying to convey.\n\nIf it is important to compare how many of the reproductions were going in each region, I would use a bar chart, maybe highlighting the top region, like in the image below.\n\n\n  \n  Bar chart, the difference between the regions is now clear and there is no need for a legend.\n\n\nIf instead is important to mark that so many different regions were receiving these reproductions I would just list the number of different end markets and compare it to the number of existing regions at that time. Or alternatively give a sentence like X% of the number of books circulating in Europe where produced from Remondini, something along the line of this\n\n\n  \n  Sometimes a sentence with the main numbers is more effective. Note: these numbers are made up\n\n\nTo summarise the nature of this data-set makes it hard to choose a pie chart, both because the number of colours is too high and because some of the slices’ sizes are so similar. The 3D effect does not help – distorting the proportion of the real numbers. Curious to see when is a good idea to use a pie chart? Have a look at my presentation on how to better tell a story with data.\n",
        "url": "/2019/06/01/a-3d-exploding-pie-chart-not-a-good-idea/"
      },
    
      {
        "title": "Querying a Snowflake Database",
        "excerpt": "My company is in the process of moving from an Amazon Redshift to a Snowflake database, and without going into the consideration of pros and cons of each, let’s see how to query data from Snowflake.\n\nUsing the Web UI\n\nThe first and easiest option is to use the web UI that comes for free once your Snowflake administrator have set you up with a username and password and given you the link to access it. You can find all the details on how to use it here.\n\nWorksheets\n\n\n  \n  \n\n\nMost notable feature of the Worksheets is that you can save, load and delete scripts and keep them saved in your area – without worrying of saving before logging-out as all the scripts are automatically saved.\n\nI wouldn’t recommend writing complex scripts in here, because you don’t have autocomplete, and you can just place the SQL element (table, column, etc.) where your cursor is placed.\n\nI would use the UI for database exploration: you can search tables by name, preview data, and see catalogs, tables and columns in your database or if do some basic row count or select all of certain tables.\n\nSnowflake is well aware of the limitations of the UI and recently acquired Numeracy, to improve it.\n\nHistory\n\n\n  \n  \n\n\nThis is a really good feature of the web UI, as you can check the performance for every user / warehouse and see what are the most expensive nodes in the execution plan of your query. This will allow you to optimise your query structure and write your query more efficiently.\n\nEvery analyst should keep an eye on the execution plan. This is also the place that tells you if in your specific query is better to use a temporary table or a CTE. Be aware of repeated executions as Snowflake caches result.\n\nUsing a SQL Client\n\nAs Snowflake UI is not very friendly for auto-complete and field suggestion, as well as formatting, you can use a SQL client of your choice to connect to the database. My go to SQL client is DBeaver, as it is open source and it supports most SQL database.\n\nSetting up DBeaver with Snowflake\n\n\n  Download the latest version, 4.3.4 or higher comes automatically with the Snowflake JDBC driver\n  Click create a New Connection and select Snowflake\n  You will need the Host, User, Password and the Role (ask your DB administrator for the details)\n  You can specify Database or the Schema, but it’s optional\n\n\n\n  \n  Connection set-up screen\n\n\nOnce you’re set up you can now navigate the database and write more complex queries with the autocomplete functionality.\n\nHope you find this useful and happy querying with Snowflake!\n",
        "content": "My company is in the process of moving from an Amazon Redshift to a Snowflake database, and without going into the consideration of pros and cons of each, let’s see how to query data from Snowflake.\n\nUsing the Web UI\n\nThe first and easiest option is to use the web UI that comes for free once your Snowflake administrator have set you up with a username and password and given you the link to access it. You can find all the details on how to use it here.\n\nWorksheets\n\n\n  \n  \n\n\nMost notable feature of the Worksheets is that you can save, load and delete scripts and keep them saved in your area – without worrying of saving before logging-out as all the scripts are automatically saved.\n\nI wouldn’t recommend writing complex scripts in here, because you don’t have autocomplete, and you can just place the SQL element (table, column, etc.) where your cursor is placed.\n\nI would use the UI for database exploration: you can search tables by name, preview data, and see catalogs, tables and columns in your database or if do some basic row count or select all of certain tables.\n\nSnowflake is well aware of the limitations of the UI and recently acquired Numeracy, to improve it.\n\nHistory\n\n\n  \n  \n\n\nThis is a really good feature of the web UI, as you can check the performance for every user / warehouse and see what are the most expensive nodes in the execution plan of your query. This will allow you to optimise your query structure and write your query more efficiently.\n\nEvery analyst should keep an eye on the execution plan. This is also the place that tells you if in your specific query is better to use a temporary table or a CTE. Be aware of repeated executions as Snowflake caches result.\n\nUsing a SQL Client\n\nAs Snowflake UI is not very friendly for auto-complete and field suggestion, as well as formatting, you can use a SQL client of your choice to connect to the database. My go to SQL client is DBeaver, as it is open source and it supports most SQL database.\n\nSetting up DBeaver with Snowflake\n\n\n  Download the latest version, 4.3.4 or higher comes automatically with the Snowflake JDBC driver\n  Click create a New Connection and select Snowflake\n  You will need the Host, User, Password and the Role (ask your DB administrator for the details)\n  You can specify Database or the Schema, but it’s optional\n\n\n\n  \n  Connection set-up screen\n\n\nOnce you’re set up you can now navigate the database and write more complex queries with the autocomplete functionality.\n\nHope you find this useful and happy querying with Snowflake!\n",
        "url": "/2019/09/29/querying-a-snowflake-database/"
      },
    
      {
        "title": "Let&#8217;s practice with this month #SWDchallenge.",
        "excerpt": "#SWDchallenge and where to find them\n\nOne of my favourite author about data visualisation, Cole N. Knaflic, published a new book Storytelling with data: Let’s Practice!\n\nI am keen of getting my own copy, but while waiting I decided to take part in this month challenge. If you’re interested too, she launch one every month on her website.\n\nOctober 2019 SWDchallenge\n\nThis is the extract from the latest post. I’ll follow the steps.\n\n\n  \n  Extract from www.storytellingwithdata.com for this challenge\n\n\nStep 1\n\nMy assumptions are:\n\n\n  The tiers are ordered by best to last (A to D)\n  As the sum of % of Accounts doesn’t sum up to 100% and neither the % Revenue – either I have missing tiers or this is the just the new clients, but then I’ve just had a really good year if my revenue growth has been so big.\n\n\nI would ask where the is the revenue missing to the 100%.\n\nStep 2\n\nThis is how I re-designed the table using Google Sheets.\n\n\n  \n  My re-desing\n\n\nHere is the list of actions, and my thoughts behind them.\n\n\n  Kept the order of the tiers, assuming the order have a meaning\n  Right align the numbers and percentage – to make the scan and comparison quicker for the reader\n  Delete background in the titles as the contrast wasn’t great for readability\n  Remove the alternate line background, as the new line separation is already enough\n  Added the unit and currency to each value in the table – to make it clear we are talking Million $, and remove it from the title as unnecessary\n  Added the Other tier – so that we have the whole and not just part of it, this is following my assumption at step 1.\n  I made the % italic, to sort of create a link between numbers and related %\n  Use the default font of Google sheet, for consistency.\n\n\nStep 3\n\nThe title of the table and the exercise suggest to look at the tier share in terms of number of accounts and revenue, and compare those two.\n\nI decided to stick with a simple visual, a column chart where I compare only the revenue and account share for the top tiers (so I excluded the Other bucket).\n\n\n  \n  Chart to compare accounts share and revenue share\n\n\nAgain I used Google Sheet, selecting the column chart and putting the two share % side to side for an easier comparison.\n\nI put the legend on the top left, to be picked up first thing by the user and kept the order of the tiers.\n\nNow having them side to side I can see how A, A+ are giving most of the revenue, while forming only 9% of all accounts. While C, D give 17% of the revenue, but has 4 times more accounts.\n\nI went for a black and grey version, bit old style and boring, but as effective – and will be the same if printed!\n\nThe link with both the data, the table and the chart can be found here. Hope you find this post useful, I will submit this on the SWDchallenge!\n",
        "content": "#SWDchallenge and where to find them\n\nOne of my favourite author about data visualisation, Cole N. Knaflic, published a new book Storytelling with data: Let’s Practice!\n\nI am keen of getting my own copy, but while waiting I decided to take part in this month challenge. If you’re interested too, she launch one every month on her website.\n\nOctober 2019 SWDchallenge\n\nThis is the extract from the latest post. I’ll follow the steps.\n\n\n  \n  Extract from www.storytellingwithdata.com for this challenge\n\n\nStep 1\n\nMy assumptions are:\n\n\n  The tiers are ordered by best to last (A to D)\n  As the sum of % of Accounts doesn’t sum up to 100% and neither the % Revenue – either I have missing tiers or this is the just the new clients, but then I’ve just had a really good year if my revenue growth has been so big.\n\n\nI would ask where the is the revenue missing to the 100%.\n\nStep 2\n\nThis is how I re-designed the table using Google Sheets.\n\n\n  \n  My re-desing\n\n\nHere is the list of actions, and my thoughts behind them.\n\n\n  Kept the order of the tiers, assuming the order have a meaning\n  Right align the numbers and percentage – to make the scan and comparison quicker for the reader\n  Delete background in the titles as the contrast wasn’t great for readability\n  Remove the alternate line background, as the new line separation is already enough\n  Added the unit and currency to each value in the table – to make it clear we are talking Million $, and remove it from the title as unnecessary\n  Added the Other tier – so that we have the whole and not just part of it, this is following my assumption at step 1.\n  I made the % italic, to sort of create a link between numbers and related %\n  Use the default font of Google sheet, for consistency.\n\n\nStep 3\n\nThe title of the table and the exercise suggest to look at the tier share in terms of number of accounts and revenue, and compare those two.\n\nI decided to stick with a simple visual, a column chart where I compare only the revenue and account share for the top tiers (so I excluded the Other bucket).\n\n\n  \n  Chart to compare accounts share and revenue share\n\n\nAgain I used Google Sheet, selecting the column chart and putting the two share % side to side for an easier comparison.\n\nI put the legend on the top left, to be picked up first thing by the user and kept the order of the tiers.\n\nNow having them side to side I can see how A, A+ are giving most of the revenue, while forming only 9% of all accounts. While C, D give 17% of the revenue, but has 4 times more accounts.\n\nI went for a black and grey version, bit old style and boring, but as effective – and will be the same if printed!\n\nThe link with both the data, the table and the chart can be found here. Hope you find this post useful, I will submit this on the SWDchallenge!\n",
        "url": "/2019/10/06/lets-practice-with-this-month-swdchallenge/"
      },
    
      {
        "title": "Snowflake vs Redshift: some syntax differences.",
        "excerpt": "Moving code from Redshift to Snowflake\n\nAs I said here, my company is in the process of moving from Redshift to Snowflake. There are loads of reasons for doing it: to mention a few, better handling of permissions, less maintenance and less effort to optimise query performance.\n\nA really good article as a starting point.\n\nI start reading around for differences between the two syntax in preparation to move lots of SQL scripts from Redshift to Snowflake, and this article explains well the major differences regarding:\n\n\n  Window functions\n  Timestamps’ operations and time-zones\n  Casing and quotes\n  General less forgiveness of Snowflake over Redshift in functions and datatype conversions\n\n\nThis article helped me a lot as starting point, to understand what could be the potential issue when the query was not running, but sometimes Snowflake error messages aren’t clear enough, and it’s hard to figure out what the issue is. So during the migration we decided to keep a log of all the differences in syntax that we found.\n\nI’ll list them below, hopefully is helpful for other people dealing with a Redshift to Snowflake migration.\n\nMore timestamps issues\n\nGETDATE() is no longer supported, but you can use CURRENT_TIMESTAMP to get the current timestamp and CURRENT_DATE to get the current date. This syntax is also compatible with Redshift so you can safely replace all your GETDATE() with CURRENT_TIMESTAMP , remembering to convert to a specific timezone if needed.\n\nDATEPART is not allowed in Snowflake, but you can use the equivalent DATE_PART, with exactly the same structure, checkout the documentation here.\n\nMore cast issues\n\nWhen casting, Snowflake is less forgiving than Redshift, but has built in functions to better deal with those, TRY_CAST that will return NULL if the conversion cannot be performed. The function can be used only if the input is of type string,  when the input is numeric use instead TRY_TO_DECIMAL, TRY_TO_NUMBER, TRY_TO_NUMERIC.\n\nThe function CONVERT does not exist any longer, but CAST is a safe alternative.\n\nBoolean and NULL\n\nWith Redshift a syntax like:\n\nCASE WHEN a is true THEN 'Yes' ELSE 'No' END\n\n\nwill work, in Snowflake is is allowed only for NULL identity verification, not for BOOLEAN, so the above should be rewritten as\n\n-- similar:\nCASE WHEN a = true THEN 'Yes' ELSE 'No' END\n\n-- using IFF:\nIFF( a, 'Yes', 'No')\n\n\nSnowflake, in fact, have an IFF function, less verbose than CASE for when only one condition need to be checked.\n\nSnowflake does not infer a BOOL type from `` and 1, so something like\n\n-- here b is a number with value 1 or 0:\nSELECT CASE WHEN b THEN 'Yes' ELSE 'No' END\n\n\nwill not work on Snowflake, you can convert to a condition on the field, so b=1 or do TO_BOOLEAN(b) or TRY_BOOLEAN(b) for a safer result.\n\nAnother difference is in the Redshift function ISNULL(a,0), this is no longer available and it needs to be replaced with IFNULL(a,0), where this returns `` when the field a is NULL. Or even better you can use COALESCE(a,0) or NVL(a,0) that are supported by both databases, and return the same result.\n\nString functions\n\nSome issues with string function might occur too. The string concatenation operator is || and + gives an error message:\n\n-- working in Snowflake:\nSELECT 'Snowflake' || ' is great' \n\n-- not working in Snowflake:\nSELECT 'Snowflake' + ' is great'\n\n\nTo remove left and right trailing spaces you would use TRIM, valid in both databases, but BTRIM that was doing the same in Redshift is not a valid function in Snowflake.\n\nThe only syntax allowed in both databases to get the length of a string is LENGTH and LEN is not valid anymore.\n\nThe equivalent of the hash function FUNC_SHA1 is SHA1 , so use this one with the same structure.\n\nParsing JSON\n\nContrary to Redshift, Snowflake allows a better handling of unstructured data so you can query JSON objects more easily. Suppose you have a JSON in the format:\n\nmy_json = '{\"f2\":\n  {\"f3\":1},\n \"f4\":\n  {\"f5\":99,\n   \"f6\":\"star\"}\n}'\n\n\nto get star from it in Redshift you would need:\n\nselect json_extract_path_text( my_json,'f4','f6')\n\n\nin Snowflake:\n\nselect parse_json( my_json:f4.f6 )\n\n\nTo know more about how to deal with JSON and semi-structured data, have a look at this document or this post in the Snowflake community.\n\nSometimes the syntax differences are hard to spot, and you end up losing a lot of time troubleshooting, a good idea is try to comment out pieces of your SQL and then test out functions and syntax in an easier example.\n\nHope you find this useful. What other differences did you spot moving your queries to Snowflake?\n",
        "content": "Moving code from Redshift to Snowflake\n\nAs I said here, my company is in the process of moving from Redshift to Snowflake. There are loads of reasons for doing it: to mention a few, better handling of permissions, less maintenance and less effort to optimise query performance.\n\nA really good article as a starting point.\n\nI start reading around for differences between the two syntax in preparation to move lots of SQL scripts from Redshift to Snowflake, and this article explains well the major differences regarding:\n\n\n  Window functions\n  Timestamps’ operations and time-zones\n  Casing and quotes\n  General less forgiveness of Snowflake over Redshift in functions and datatype conversions\n\n\nThis article helped me a lot as starting point, to understand what could be the potential issue when the query was not running, but sometimes Snowflake error messages aren’t clear enough, and it’s hard to figure out what the issue is. So during the migration we decided to keep a log of all the differences in syntax that we found.\n\nI’ll list them below, hopefully is helpful for other people dealing with a Redshift to Snowflake migration.\n\nMore timestamps issues\n\nGETDATE() is no longer supported, but you can use CURRENT_TIMESTAMP to get the current timestamp and CURRENT_DATE to get the current date. This syntax is also compatible with Redshift so you can safely replace all your GETDATE() with CURRENT_TIMESTAMP , remembering to convert to a specific timezone if needed.\n\nDATEPART is not allowed in Snowflake, but you can use the equivalent DATE_PART, with exactly the same structure, checkout the documentation here.\n\nMore cast issues\n\nWhen casting, Snowflake is less forgiving than Redshift, but has built in functions to better deal with those, TRY_CAST that will return NULL if the conversion cannot be performed. The function can be used only if the input is of type string,  when the input is numeric use instead TRY_TO_DECIMAL, TRY_TO_NUMBER, TRY_TO_NUMERIC.\n\nThe function CONVERT does not exist any longer, but CAST is a safe alternative.\n\nBoolean and NULL\n\nWith Redshift a syntax like:\n\nCASE WHEN a is true THEN 'Yes' ELSE 'No' END\n\n\nwill work, in Snowflake is is allowed only for NULL identity verification, not for BOOLEAN, so the above should be rewritten as\n\n-- similar:\nCASE WHEN a = true THEN 'Yes' ELSE 'No' END\n\n-- using IFF:\nIFF( a, 'Yes', 'No')\n\n\nSnowflake, in fact, have an IFF function, less verbose than CASE for when only one condition need to be checked.\n\nSnowflake does not infer a BOOL type from `` and 1, so something like\n\n-- here b is a number with value 1 or 0:\nSELECT CASE WHEN b THEN 'Yes' ELSE 'No' END\n\n\nwill not work on Snowflake, you can convert to a condition on the field, so b=1 or do TO_BOOLEAN(b) or TRY_BOOLEAN(b) for a safer result.\n\nAnother difference is in the Redshift function ISNULL(a,0), this is no longer available and it needs to be replaced with IFNULL(a,0), where this returns `` when the field a is NULL. Or even better you can use COALESCE(a,0) or NVL(a,0) that are supported by both databases, and return the same result.\n\nString functions\n\nSome issues with string function might occur too. The string concatenation operator is || and + gives an error message:\n\n-- working in Snowflake:\nSELECT 'Snowflake' || ' is great' \n\n-- not working in Snowflake:\nSELECT 'Snowflake' + ' is great'\n\n\nTo remove left and right trailing spaces you would use TRIM, valid in both databases, but BTRIM that was doing the same in Redshift is not a valid function in Snowflake.\n\nThe only syntax allowed in both databases to get the length of a string is LENGTH and LEN is not valid anymore.\n\nThe equivalent of the hash function FUNC_SHA1 is SHA1 , so use this one with the same structure.\n\nParsing JSON\n\nContrary to Redshift, Snowflake allows a better handling of unstructured data so you can query JSON objects more easily. Suppose you have a JSON in the format:\n\nmy_json = '{\"f2\":\n  {\"f3\":1},\n \"f4\":\n  {\"f5\":99,\n   \"f6\":\"star\"}\n}'\n\n\nto get star from it in Redshift you would need:\n\nselect json_extract_path_text( my_json,'f4','f6')\n\n\nin Snowflake:\n\nselect parse_json( my_json:f4.f6 )\n\n\nTo know more about how to deal with JSON and semi-structured data, have a look at this document or this post in the Snowflake community.\n\nSometimes the syntax differences are hard to spot, and you end up losing a lot of time troubleshooting, a good idea is try to comment out pieces of your SQL and then test out functions and syntax in an easier example.\n\nHope you find this useful. What other differences did you spot moving your queries to Snowflake?\n",
        "url": "/2019/10/13/snowflake-vs-redshift-some-syntax-differences/"
      },
    
      {
        "title": "Aliasing",
        "excerpt": "I’ll be starting a series of posts about some cool Snowflake (The cloud data platform) features, that makes SnowSQL really cool – compared to other SQL dialect (I am most familiar with Microsoft SQL Server or T-SQL).\n\nI’ll start the series, talking about aliasing.\n\nSometimes our SQL get really complex: who haven’t seen a nested case statement grows exponentially? so much that after a while you can’t understand anymore if the output is what is supposed to be, and same for window functions.\n\nThe suggestion when complexity arise is usually “break down the problem in smaller steps”, and that’s where aliasing can help the most writing your SQL queries.\n\nSnowflake features of aliasing allows you to re-use the same SQL block in another column definition and simplify massively your code, making it more understandable.\n\nFor example you might have a SQL like this:\n\nselect \n  case \n    when price &gt; 100 \n      and referral_code  = 'XXX1' then price * 0.9\n    when referral_code = 'XX2' then price*0.75\n    when referral_code = 'XX3' then price - 10 \n    else price end as discounted_price,\n  case \n    when partner = 'A' then discounted_price*0.2\n    when partner = 'B' then discounted_price*0.3\n    else discounted_price end as partner_fee\nfrom my_table\n\n\nSo this is the new alternative, without the aliasing capability (for example in a database like SQL Server, you would have to explicitly re-write the logic for the discounted_price twice (not DRY, Don’t Repeat Yourself) or create a function in the database (like this). So much neater using the alias!\n\nMoreover you can use those aliases in the where, group by, having, qualify and order by statement, saving you from copying the same logic multiple times in your query.\n\nHere is another example:\n\nselect \n  first_name,\n  last_name,\n  first_name || ' ' || last_name as full_name\nfrom names\nwhere full_name = 'My Full Name'\n\n\nIn general this feature is amazing, but there are a couple cases where it might trick you. In particular: alias don’t override existing columns names and can have unexpected behaviour when used in join clauses.\n\nAlias conflicting with existing columns names\n\nSnowflake doesn’t infer the alias if you create an alias that is already a columns in one of the tables you join. Suppose the tables look like these:\n\nfirst table t1\n\n\n  \n    \n      id\n      email_address\n    \n  \n  \n    \n      12\n      foo@com\n    \n    \n      13\n      hola@com\n    \n  \n\n\n\nsecond table t2:\n\n\n  \n    \n      my_id\n      email_address\n    \n  \n  \n    \n      12\n      food@com\n    \n  \n\n\n\nThe following query\n\nselect \n  t1.id\n  , coalesce( t2.email_address, t1.email_address) as email_address\nfrom t1\nleft join t2\n  on t1.id = t2.my_id\nwhere email_address = 'foo@com'\n\n\nwill give you an error of ambiguous column name, so you’ll need to specify the table for the column you want to use or rename the column alias to be something different from the column names already existing in your tables.\n\nAlias used in joins.\n\nSnowflake doesn’t like aliases in joins, suppose we have two tables above, and say you want to rename the my_id of the second table to new_id :\n\nselect \n  t1.id\n  ,t2.my_id as new_id\nfrom t1\nleft join t2 \n  on t1.id = t2.new_id\n\n\nWill give you an error of unknown column names, so careful when using aliases in the join.\n\nTo summarise, aliases are amazing to simplify your code and stay DRY but needs to be used with care when joining tables.\n\nHope you too loves aliases, or start using them after reading this post! Did you find any other troubles using them? Let me know.\n",
        "content": "I’ll be starting a series of posts about some cool Snowflake (The cloud data platform) features, that makes SnowSQL really cool – compared to other SQL dialect (I am most familiar with Microsoft SQL Server or T-SQL).\n\nI’ll start the series, talking about aliasing.\n\nSometimes our SQL get really complex: who haven’t seen a nested case statement grows exponentially? so much that after a while you can’t understand anymore if the output is what is supposed to be, and same for window functions.\n\nThe suggestion when complexity arise is usually “break down the problem in smaller steps”, and that’s where aliasing can help the most writing your SQL queries.\n\nSnowflake features of aliasing allows you to re-use the same SQL block in another column definition and simplify massively your code, making it more understandable.\n\nFor example you might have a SQL like this:\n\nselect \n  case \n    when price &gt; 100 \n      and referral_code  = 'XXX1' then price * 0.9\n    when referral_code = 'XX2' then price*0.75\n    when referral_code = 'XX3' then price - 10 \n    else price end as discounted_price,\n  case \n    when partner = 'A' then discounted_price*0.2\n    when partner = 'B' then discounted_price*0.3\n    else discounted_price end as partner_fee\nfrom my_table\n\n\nSo this is the new alternative, without the aliasing capability (for example in a database like SQL Server, you would have to explicitly re-write the logic for the discounted_price twice (not DRY, Don’t Repeat Yourself) or create a function in the database (like this). So much neater using the alias!\n\nMoreover you can use those aliases in the where, group by, having, qualify and order by statement, saving you from copying the same logic multiple times in your query.\n\nHere is another example:\n\nselect \n  first_name,\n  last_name,\n  first_name || ' ' || last_name as full_name\nfrom names\nwhere full_name = 'My Full Name'\n\n\nIn general this feature is amazing, but there are a couple cases where it might trick you. In particular: alias don’t override existing columns names and can have unexpected behaviour when used in join clauses.\n\nAlias conflicting with existing columns names\n\nSnowflake doesn’t infer the alias if you create an alias that is already a columns in one of the tables you join. Suppose the tables look like these:\n\nfirst table t1\n\n\n  \n    \n      id\n      email_address\n    \n  \n  \n    \n      12\n      foo@com\n    \n    \n      13\n      hola@com\n    \n  \n\n\n\nsecond table t2:\n\n\n  \n    \n      my_id\n      email_address\n    \n  \n  \n    \n      12\n      food@com\n    \n  \n\n\n\nThe following query\n\nselect \n  t1.id\n  , coalesce( t2.email_address, t1.email_address) as email_address\nfrom t1\nleft join t2\n  on t1.id = t2.my_id\nwhere email_address = 'foo@com'\n\n\nwill give you an error of ambiguous column name, so you’ll need to specify the table for the column you want to use or rename the column alias to be something different from the column names already existing in your tables.\n\nAlias used in joins.\n\nSnowflake doesn’t like aliases in joins, suppose we have two tables above, and say you want to rename the my_id of the second table to new_id :\n\nselect \n  t1.id\n  ,t2.my_id as new_id\nfrom t1\nleft join t2 \n  on t1.id = t2.new_id\n\n\nWill give you an error of unknown column names, so careful when using aliases in the join.\n\nTo summarise, aliases are amazing to simplify your code and stay DRY but needs to be used with care when joining tables.\n\nHope you too loves aliases, or start using them after reading this post! Did you find any other troubles using them? Let me know.\n",
        "url": "/2020/08/24/aliasing/"
      },
    
      {
        "title": "Qualify",
        "excerpt": "This is the second post about some Snowflake features, that I use often and find useful to write a better SQL. You can find the first of the series about aliasing here.\n\nWhat is it?\n\nHave you ever been in the situation where you want to keep all rows of your query but than filter the result set based on a condition you would get from a subset of it? Let me clarify this with some examples:\n\n\n  Suppose you are tracking all the interactions with a website and you want to see what happened after a specific action took place: then you would want to keep all events for those customers that have a specific event in their event stream.\n  Suppose you sell multiple products per customers and you want to see all the purchase history of the customers that have a very specific product as last purchase.\n\n\nLet’s stick with this second example. In that case you would have a table looking like this, let’s call it purchases:\n\n\n  \n    \n      customer_id\n      product\n      purchased_date (YYYY-MM-DD)\n    \n  \n  \n    \n      1\n      A\n      2020-04-01\n    \n    \n      1\n      B\n      2020-05-03\n    \n    \n      2\n      A\n      2020-03-01\n    \n    \n      3\n      B\n      2020-04-01\n    \n  \n\n\n\nSo customer 1 and 3 last purchase was product B, and customer 2 last purchase was product A. Now suppose you want to filter in your data only the customers that purchased B as last product, for an analysis you’re doing.\n\nWhat you would do, without using the qualify statement, would be something like this:\n\nwith product_purchased_order as (\nselect \n  customer_id\n  , product\n  , row_number() over (partition by customer_id \n      order by purchased_date desc) as n_last_order_nr\nfrom purchases\n)\nselect customer_id \nfrom product_purchased_order\nwhere n_last_order_nr = 1 and product ='B'\n\n\nYou see that I have to use at least two queries: first the window statement and the where clause to filter the results I want. There are many other way to get to the same result, but let’s see how to simplify the query.\n\nThe qualify expression allows to rewrite the statement above to read nicely and in one single query. Here you go:\n\nselect \n  customer_id\n  , product \n  , last_value( product ) over (partition by customer_id \n      order by purchased_date) as last_product_purchased\nfrom purchases\nqualify last_product_purchased = 'B'\n\n\nNote the benefits of using this second option:\n\n\n  I can write only one query\n  I can keep in the same query all the fields in the base table: in this case all the purchase history, even though I am just keeping the customer that have bought B as last product – I can still see all the products they purchased.\n\n\nWhen analysing something, you often want to filter by a condition on a partition, but keep all the records at a lower granularity – that is where the qualify come in handy. Note also how using the alias in the query above made the statement more clear.\n\nWhere does the qualify clause this fit?\n\nFrom the Snowflake general query syntax reference you get this general outline:\n\n[ WITH ... ]\nSELECT\n   [ TOP &lt;n&gt; ]\n   ...\n[ FROM ...\n   [ AT | BEFORE ... ]\n   [ CHANGES ... ]\n   [ CONNECT BY ... ]\n   [ JOIN ... ]\n   [ MATCH_RECOGNIZE ... ]\n   [ PIVOT | UNPIVOT ... ]\n   [ VALUES ... ]\n   [ SAMPLE ... ] ]\n[ WHERE ... ]\n[ GROUP BY ...\n   [ HAVING ... ] ]\n[ QUALIFY ... ]\n[ ORDER BY ... ]\n[ LIMIT ... ]\n\n\nSo, the qualify sit in the query order right after the having (if present) but before the order by and the limit clauses.\n\nIt’s also possible to not include the window function you are filtering by in the select list, which is good in case you don’t want to show the value you are filtering on, for example in the previous query something like:\n\nselect \n  customer_id\n  , product\nfrom purchases\nqualify last_value( product ) over (partition by customer_id \n    order by purchased_date) = 'B'\n\n\nWhich could be good if the additional field is redundant or used just for filtering.\n\nThat’s it for today! Hope you’ll start getting rid of some extra CTEs / sub-queries, thanks to the qualify or, why not, have better time writing the logic to get some fancy slice of your data, based on whatever complex condition.\n\nHave you used the qualify statement? What are the pros and cons about it? Any warnings that you want to share? Let me know!\n",
        "content": "This is the second post about some Snowflake features, that I use often and find useful to write a better SQL. You can find the first of the series about aliasing here.\n\nWhat is it?\n\nHave you ever been in the situation where you want to keep all rows of your query but than filter the result set based on a condition you would get from a subset of it? Let me clarify this with some examples:\n\n\n  Suppose you are tracking all the interactions with a website and you want to see what happened after a specific action took place: then you would want to keep all events for those customers that have a specific event in their event stream.\n  Suppose you sell multiple products per customers and you want to see all the purchase history of the customers that have a very specific product as last purchase.\n\n\nLet’s stick with this second example. In that case you would have a table looking like this, let’s call it purchases:\n\n\n  \n    \n      customer_id\n      product\n      purchased_date (YYYY-MM-DD)\n    \n  \n  \n    \n      1\n      A\n      2020-04-01\n    \n    \n      1\n      B\n      2020-05-03\n    \n    \n      2\n      A\n      2020-03-01\n    \n    \n      3\n      B\n      2020-04-01\n    \n  \n\n\n\nSo customer 1 and 3 last purchase was product B, and customer 2 last purchase was product A. Now suppose you want to filter in your data only the customers that purchased B as last product, for an analysis you’re doing.\n\nWhat you would do, without using the qualify statement, would be something like this:\n\nwith product_purchased_order as (\nselect \n  customer_id\n  , product\n  , row_number() over (partition by customer_id \n      order by purchased_date desc) as n_last_order_nr\nfrom purchases\n)\nselect customer_id \nfrom product_purchased_order\nwhere n_last_order_nr = 1 and product ='B'\n\n\nYou see that I have to use at least two queries: first the window statement and the where clause to filter the results I want. There are many other way to get to the same result, but let’s see how to simplify the query.\n\nThe qualify expression allows to rewrite the statement above to read nicely and in one single query. Here you go:\n\nselect \n  customer_id\n  , product \n  , last_value( product ) over (partition by customer_id \n      order by purchased_date) as last_product_purchased\nfrom purchases\nqualify last_product_purchased = 'B'\n\n\nNote the benefits of using this second option:\n\n\n  I can write only one query\n  I can keep in the same query all the fields in the base table: in this case all the purchase history, even though I am just keeping the customer that have bought B as last product – I can still see all the products they purchased.\n\n\nWhen analysing something, you often want to filter by a condition on a partition, but keep all the records at a lower granularity – that is where the qualify come in handy. Note also how using the alias in the query above made the statement more clear.\n\nWhere does the qualify clause this fit?\n\nFrom the Snowflake general query syntax reference you get this general outline:\n\n[ WITH ... ]\nSELECT\n   [ TOP &lt;n&gt; ]\n   ...\n[ FROM ...\n   [ AT | BEFORE ... ]\n   [ CHANGES ... ]\n   [ CONNECT BY ... ]\n   [ JOIN ... ]\n   [ MATCH_RECOGNIZE ... ]\n   [ PIVOT | UNPIVOT ... ]\n   [ VALUES ... ]\n   [ SAMPLE ... ] ]\n[ WHERE ... ]\n[ GROUP BY ...\n   [ HAVING ... ] ]\n[ QUALIFY ... ]\n[ ORDER BY ... ]\n[ LIMIT ... ]\n\n\nSo, the qualify sit in the query order right after the having (if present) but before the order by and the limit clauses.\n\nIt’s also possible to not include the window function you are filtering by in the select list, which is good in case you don’t want to show the value you are filtering on, for example in the previous query something like:\n\nselect \n  customer_id\n  , product\nfrom purchases\nqualify last_value( product ) over (partition by customer_id \n    order by purchased_date) = 'B'\n\n\nWhich could be good if the additional field is redundant or used just for filtering.\n\nThat’s it for today! Hope you’ll start getting rid of some extra CTEs / sub-queries, thanks to the qualify or, why not, have better time writing the logic to get some fancy slice of your data, based on whatever complex condition.\n\nHave you used the qualify statement? What are the pros and cons about it? Any warnings that you want to share? Let me know!\n",
        "url": "/2020/08/26/qualify/"
      },
    
      {
        "title": "Why I love dbt",
        "excerpt": "I am an analyst and since the early days I’ve found quite frustrating having to wait days, weeks or even months for the data to be available – because the data warehouse developers were busy, and felt I couldn’t add value or contribute doing what I knew: analysing data.\n\nAnd I still remember my first job as a BI developer back in 2014 where I was using Oracle BI, and I needed to wait the transformation and loading to be completed, before I could do anything at all. That implies a waste not only because the BI developers can’t use their expertise in helping the design, but also because the agility of creating data value is greatly compromised (more on this in the section Coupled pipeline decomposition of this article).\n\nFast forward March 2019, my team at Simply Business decided to introduce dbt, in wander for a more clear pipeline and a more robust analytics. The new paradigm is ELT (Extract, Load, Transform) instead of ETL (Extract, Transform, Load) because of the incredible computing power the cloud databases (Snowflake, Redshift, BigQuery among others) can bring to the table.\n\nWith this new approach, I no longer need to wait for someone to create the base tables for me to use in the reporting tool of choice, and what is most exciting I have a lot more freedom and flexibility to tailor my data structure to the needs of the reporting tool and the business requirements, iterating faster than ever before.\n\nWhat it means is my role as well changed, and I am no longer building reports / reporting models waiting patiently for someone to load the clean and modelled data for me, but I can just wait for someone to load them and then rework and adapt the model to the business requirements, exposing the changes in the reporting layer with no waiting times. Some companies went on and even adopted some loaders like Stitch or FiveTran to be able to load data from the data sources and make the loading even quicker.\n\nThis change means two main consequences on my role: more knowledge of the data sources and the relationship among them, but also the responsibility (with more power comes more responsibility, the evergreen quote) to create something that is reusable rather than create a new table every time a new requirement come in: so understanding if it’s a case of adding to the existing content or create something else to cover the new business questions arising.\n\n\n  \n  My bitmoji, credit to Bitmoji &amp; dbt for the logo. I feel a super-analyst, just thanks to dbt.\n\n\ndbt is the T, the Trasform, in the ELT process, and the power that made me a super-analyst: faster, more reliable and empowered to understand not only the business, but how the business needs are translated into a data model. That’s why I love dbt and I’ll try to describe it in some upcoming posts, hoping to spread the love.\n\nCurious to know more? Start reading these blog posts:\n\n\n  What, exactly, is dbt?\n  Is dbt the right tool for my data transformations?\n  Why dbt is so important?\n\n\nFeel like you want to listen to something, instead? [Trying to avoid screen time uh?] Try this podcast episode:\n\nSoftware Engineering Daily, DBT: Data Build Tool with Tristan Handy\n\nWant to learn by doing? Check out this Tutorial\n\nWant to support your learning? The dbt community is just incredibly helpful and welcoming, you can join the slack channel, or check out the Discourse online.\n\nAnd finally they just re-launched Coalesce 2020, 7-11 December, can’t wait to attend!\n",
        "content": "I am an analyst and since the early days I’ve found quite frustrating having to wait days, weeks or even months for the data to be available – because the data warehouse developers were busy, and felt I couldn’t add value or contribute doing what I knew: analysing data.\n\nAnd I still remember my first job as a BI developer back in 2014 where I was using Oracle BI, and I needed to wait the transformation and loading to be completed, before I could do anything at all. That implies a waste not only because the BI developers can’t use their expertise in helping the design, but also because the agility of creating data value is greatly compromised (more on this in the section Coupled pipeline decomposition of this article).\n\nFast forward March 2019, my team at Simply Business decided to introduce dbt, in wander for a more clear pipeline and a more robust analytics. The new paradigm is ELT (Extract, Load, Transform) instead of ETL (Extract, Transform, Load) because of the incredible computing power the cloud databases (Snowflake, Redshift, BigQuery among others) can bring to the table.\n\nWith this new approach, I no longer need to wait for someone to create the base tables for me to use in the reporting tool of choice, and what is most exciting I have a lot more freedom and flexibility to tailor my data structure to the needs of the reporting tool and the business requirements, iterating faster than ever before.\n\nWhat it means is my role as well changed, and I am no longer building reports / reporting models waiting patiently for someone to load the clean and modelled data for me, but I can just wait for someone to load them and then rework and adapt the model to the business requirements, exposing the changes in the reporting layer with no waiting times. Some companies went on and even adopted some loaders like Stitch or FiveTran to be able to load data from the data sources and make the loading even quicker.\n\nThis change means two main consequences on my role: more knowledge of the data sources and the relationship among them, but also the responsibility (with more power comes more responsibility, the evergreen quote) to create something that is reusable rather than create a new table every time a new requirement come in: so understanding if it’s a case of adding to the existing content or create something else to cover the new business questions arising.\n\n\n  \n  My bitmoji, credit to Bitmoji &amp; dbt for the logo. I feel a super-analyst, just thanks to dbt.\n\n\ndbt is the T, the Trasform, in the ELT process, and the power that made me a super-analyst: faster, more reliable and empowered to understand not only the business, but how the business needs are translated into a data model. That’s why I love dbt and I’ll try to describe it in some upcoming posts, hoping to spread the love.\n\nCurious to know more? Start reading these blog posts:\n\n\n  What, exactly, is dbt?\n  Is dbt the right tool for my data transformations?\n  Why dbt is so important?\n\n\nFeel like you want to listen to something, instead? [Trying to avoid screen time uh?] Try this podcast episode:\n\nSoftware Engineering Daily, DBT: Data Build Tool with Tristan Handy\n\nWant to learn by doing? Check out this Tutorial\n\nWant to support your learning? The dbt community is just incredibly helpful and welcoming, you can join the slack channel, or check out the Discourse online.\n\nAnd finally they just re-launched Coalesce 2020, 7-11 December, can’t wait to attend!\n",
        "url": "/2020/09/03/why-i-love-dbt/"
      },
    
      {
        "title": "Forecasting with Facebook Prophet, an introduction.",
        "excerpt": "Time series analysis and forecasting are useful in so many contexts, from economy to demographics, and in all the scenarios where you have a sequence of observations that are indexed over time. Examples include the number of Covid-19 cases per day (sadly we are all too familiar with this), the number of pizzas served in a day by a restaurant (this makes me happier), or again the average daily temperature.\n\nOften in business scenarios, there is a need for forecasting demand, orders, or revenue, and sometimes just assuming a linear pattern doesn’t account for seasonality and change in trend. Plus having a quicker and more reliable way to forecast allows businesses to adapt faster to change - improving their planning and spending.\n\nForecasting has proven especially challenging during Covid-19, as the pandemic was totally unpredictable and made planning for 2020 and beyond really hard. Nonetheless forecasting is useful to have an idea of how the future might look like, given the history.\n\nRemember that\n\n\n  Past performance is no guarantee of future results\n\n\nbut using Prophet you feel a little bit like:\n\n\nvia GIPHY\n\nWhat is Facebook Prophet?\n\nFacebook Prophet it’s an open-source project first released in February 2017, that offers a forecasting procedure implemented with R and Python. It provides the ability to use human-interpretable parameters to improve the forecasts by adding your domain knowledge. The documentation, tutorials, and all relevant links to the code repository are available on the project website.\n\n\n  \n  Prophet project page homepage\n\n\nHow does it work?\n\nMore information can be found in this paper, but this is what the procedure is trying to do. Suppose we have a time-series y(t):\n\n  y(t)= g(t) + s(t) + h(t) + ε(t) \n\n\nwith those components:\n\n\n  a trend g(t) that capture non-periodic changes\n  a seasonal component s(t), which models for example weekly or yearly seasonality\n  a holiday component h(t), to account for the effect of holidays or other occurrences with potentially a more irregular schedule.\n\n\nUsing time as a regressor Prophet is trying to fit several linear and non linear functions of time as components, minimising the error ε(t).\n\nWhat are the benefits?\n\nAccording to the website the main advantages are:\n\n\n  Simplify the approach to forecasting, as is robust to outliers, missing data, and dramatic changes in your time series\n  The model fitting uses Stan, which improve performance\n  It allows performance tuning using insight coming from the domain knowledge\n  It’s available in both R and Python and completely open-source.\n\n\nI’ll be using Python, as is the language I am more familiar with.\n\nSuccess stories of using Prophet to scale forecasting, include Facebook themselves, but also Starbucks.\n\nThe steps\n\nThese are the steps you generally need to follow if you want to generate a forecast for a time series using Prophet, with Python:\n\n\n  Setup. Have an installation of Python 3 in your laptop compatible with Prophet\n  Installation. Install Facebook Prophet\n  Time Series. Have a time series in hand, in the form of a DataFrame (Python equivalent of a table) with one column for time and one for the value of the series at that point in time\n  Exploratory Data Analysis. Analyse seasonality, holiday effects, or eventual segmentation of your series.\n  Predict. Try out with forecasting with the default setting\n  Diagnose. Evaluate model performance, with the in-build cross-validation function\n  Optimise. Tune parameters and re-evaluate. Usually, you predict, validate, and tune in a loop, until you’re happy with the error.\n\n\nI’ll show an example following these steps in another post.\n",
        "content": "Time series analysis and forecasting are useful in so many contexts, from economy to demographics, and in all the scenarios where you have a sequence of observations that are indexed over time. Examples include the number of Covid-19 cases per day (sadly we are all too familiar with this), the number of pizzas served in a day by a restaurant (this makes me happier), or again the average daily temperature.\n\nOften in business scenarios, there is a need for forecasting demand, orders, or revenue, and sometimes just assuming a linear pattern doesn’t account for seasonality and change in trend. Plus having a quicker and more reliable way to forecast allows businesses to adapt faster to change - improving their planning and spending.\n\nForecasting has proven especially challenging during Covid-19, as the pandemic was totally unpredictable and made planning for 2020 and beyond really hard. Nonetheless forecasting is useful to have an idea of how the future might look like, given the history.\n\nRemember that\n\n\n  Past performance is no guarantee of future results\n\n\nbut using Prophet you feel a little bit like:\n\n\nvia GIPHY\n\nWhat is Facebook Prophet?\n\nFacebook Prophet it’s an open-source project first released in February 2017, that offers a forecasting procedure implemented with R and Python. It provides the ability to use human-interpretable parameters to improve the forecasts by adding your domain knowledge. The documentation, tutorials, and all relevant links to the code repository are available on the project website.\n\n\n  \n  Prophet project page homepage\n\n\nHow does it work?\n\nMore information can be found in this paper, but this is what the procedure is trying to do. Suppose we have a time-series y(t):\n\n  y(t)= g(t) + s(t) + h(t) + ε(t) \n\n\nwith those components:\n\n\n  a trend g(t) that capture non-periodic changes\n  a seasonal component s(t), which models for example weekly or yearly seasonality\n  a holiday component h(t), to account for the effect of holidays or other occurrences with potentially a more irregular schedule.\n\n\nUsing time as a regressor Prophet is trying to fit several linear and non linear functions of time as components, minimising the error ε(t).\n\nWhat are the benefits?\n\nAccording to the website the main advantages are:\n\n\n  Simplify the approach to forecasting, as is robust to outliers, missing data, and dramatic changes in your time series\n  The model fitting uses Stan, which improve performance\n  It allows performance tuning using insight coming from the domain knowledge\n  It’s available in both R and Python and completely open-source.\n\n\nI’ll be using Python, as is the language I am more familiar with.\n\nSuccess stories of using Prophet to scale forecasting, include Facebook themselves, but also Starbucks.\n\nThe steps\n\nThese are the steps you generally need to follow if you want to generate a forecast for a time series using Prophet, with Python:\n\n\n  Setup. Have an installation of Python 3 in your laptop compatible with Prophet\n  Installation. Install Facebook Prophet\n  Time Series. Have a time series in hand, in the form of a DataFrame (Python equivalent of a table) with one column for time and one for the value of the series at that point in time\n  Exploratory Data Analysis. Analyse seasonality, holiday effects, or eventual segmentation of your series.\n  Predict. Try out with forecasting with the default setting\n  Diagnose. Evaluate model performance, with the in-build cross-validation function\n  Optimise. Tune parameters and re-evaluate. Usually, you predict, validate, and tune in a loop, until you’re happy with the error.\n\n\nI’ll show an example following these steps in another post.\n",
        "url": "/2021/02/07/forecasting-with-facebook-prophet-an-introduction/"
      },
    
  
  
  
  {
    "title": "Categories",
    "excerpt": "Category index\n",
    "content": "{% include site-search.html %}\n",
    "url": "/categories/"
  },
  
  {
    "title": "Elements",
    "excerpt": "A demo of Markdown and HTML includes\n",
    "content": "Heading 1\n\nHeading 2\n\nHeading 3\n\nHeading 4\n\nHeading 5\n\nHeading 6\n\nA small element\n\nA link\n\nLorem ipsum dolor sit amet, consectetur adip* isicing elit, sed do eiusmod *tempor incididunt ut labore et dolore magna aliqua.\n\nDuis aute irure dolor in A link reprehenderit in voluptate velit esse cillum bold text dolore eu fugiat nulla pariatur. Excepteur span element sint occaecat cupidatat non proident, sunt italicised text in culpa qui officia deserunt mollit anim id some code est laborum.\n\n\n  An item\n  An item\n  An item\n  An item\n  An item\n\n\n\n  Item one\n  Item two\n  Item three\n  Item four\n  Item five\n\n\n\n  A simple blockquote\n\n\nSome HTML…\n\n&lt;blockquote cite=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;\n  &lt;p&gt;You planning a vacation, Mr. Sullivan?&lt;/p&gt;\n  &lt;footer&gt;\n    &lt;a href=\"http://www.imdb.com/title/tt0284978/quotes/qt1375101\"&gt;Sunways Security Guard&lt;/a&gt;\n  &lt;/footer&gt;\n&lt;/blockquote&gt;\n\n\n…CSS…\n\nblockquote {\n  text-align: center;\n  font-weight: bold;\n}\nblockquote footer {\n  font-size: .8rem;\n}\n\n\n…and JavaScript\n\nconst blockquote = document.querySelector(\"blockquote\")\nconst bolden = (keyString, string) =&gt;\n  string.replace(new RegExp(keyString, 'g'), '&lt;strong&gt;'+keyString+'&lt;/strong&gt;')\n\nblockquote.innerHTML = bolden(\"Mr. Sullivan\", blockquote.innerHTML)\n\n\nSingle line of code\n\nHTML Includes\n\nContact form\n\n{% include site-form.html %}\n\n{% raw %}{% include site-form.html %}{% endraw %}\n\n\nDemo map embed\n\n{% include map.html id=”1UT-2Z-Vg_MG_TrS5X2p8SthsJhc” title=”Coffee shop map” %}\n\n{% raw %}{% include map.html id=\"XXXXXX\" title=\"Coffee shop map\" %}{% endraw %}\n\n\nButton include\n\n{% include button.html text=”A button” link=”https://david.darn.es” %}\n\n{% include button.html text=”A button with icon” link=”https://twitter.com/daviddarnes” icon=”twitter” %}\n\n{% raw %}{% include button.html text=\"A button\" link=\"https://david.darn.es\" %}\n{% include button.html text=\"A button with icon\" link=\"https://twitter.com/daviddarnes\" icon=\"twitter\" %}{% endraw %}\n\n\nIcon include\n\n{% include icon.html id=”twitter” title=”twitter” %} {% include icon.html id=”linkedin” title=”twitter” %}\n\n{% raw %}{% include icon.html id=\"twitter\" title=\"twitter\" %}\n[{% include icon.html id=\"linkedin\" title=\"twitter\" %}](https://www.linkedin.com/in/daviddarnes){% endraw %}\n\n\nVideo include\n\n{% include video.html id=”zrkcGL5H3MU” title=”Siteleaf tutorial video” %}\n\n{% raw %}{% include video.html id=\"zrkcGL5H3MU\" title=\"Siteleaf tutorial video\" %}{% endraw %}\n\n\nImage includes\n\n{% include figure.html image=”https://picsum.photos/600/800?image=894” caption=”Image with caption” width=”300” height=”800” %}\n\n{% include figure.html image=”https://picsum.photos/600/800?image=894” caption=”Right aligned image” position=”right” width=”300” height=”800” %}\n\n{% include figure.html image=”https://picsum.photos/600/800?image=894” caption=”Left aligned image” position=”left” width=”300” height=”800” %}\n\n{% include figure.html image=”https://picsum.photos/1600/800?image=894” alt=”Image with just alt text” %}\n\n{% raw %}{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Image with caption\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Right aligned image\" position=\"right\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/600/800?image=894\" caption=\"Left aligned image\" position=\"left\" width=\"300\" height=\"800\" %}\n\n{% include figure.html image=\"https://picsum.photos/1600/800?image=894\" alt=\"Image with just alt text\" %}{% endraw %}\n\n",
    "url": "/elements/"
  },
  
  {
    "title": "About Food For Data",
    "excerpt": "\n",
    "content": "I am Martina, a business intelligence analyst, based in London.\n\nI have a background in Mathematics, and always loved its abstraction and power to formalise complex quantitative problems.\n\nI love data visualisation, because it brings together creativity, design and science.\n\nThis blog is a way to share my thoughts around data, hoping to sparkle some interesting considerations around data analytics.\n\nI am originally from Italy and a foodie, so this name couldn’t fit more.\n",
    "url": "/"
  },
  
  {
    "title": "Search",
    "excerpt": "Search for a page or post you’re looking for\n",
    "content": "{% include site-search.html %}\n",
    "url": "/search/"
  }
  
]

